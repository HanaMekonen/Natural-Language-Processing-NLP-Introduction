# Import required libraries
# Import required libraries
import nltk
import spacy
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import numpy as np
import pandas as pd
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import ngrams
nlp = spacy.load("en_core_web_sm")

### 1. Text Preprocessing
#### Clean the following text by converting it to lowercase, removing punctuation and stop words.

import string

# Sample text
text = "Natural Language Processing is a fascinating field. It combines linguistics and computer science!"

# TO: Preprocess the text
def preprocess(text):
    # Convert to lowercase
    text_lower = text.lower()
    # Tokenize
    doc = nlp(text_lower)
    tokens = [token.text for token in doc]
    # Remove punctuation and stopwords
    tokens = [token for token in tokens if token not in string.punctuation]
    stop_words = set(stopwords.words('english'))
    cleaned_tokens = [token for token in tokens if token not in stop_words]
    return cleaned_tokens

# Print cleaned tokens
cleaned_tokens = preprocess(text)
print(cleaned_tokens)

### 2. Tokenization and N-grams
#### Generate bigrams (2-grams) from the cleaned tokens.

# Generate bigrams from cleaned tokens
bigrams = list(ngrams(cleaned_tokens, 2))
print("Bigrams:", bigrams)

#One more example by merging the above 2 topics.
# Sample text
text = "Natural language processing is exciting"

# Tokenize
doc = nlp(text.lower())
tokens = [token.text for token in doc]
# Remove punctuation (optional)
tokens = [t for t in tokens if t not in string.punctuation]

# generate bigrams
bi_grams = list(bigrams(tokens))

# Print results
print("Tokens:", tokens)
print("Bigrams:", bi)

### 3. Named Entity Recognition (NER)
#### Use spaCy to perform NER on a new sentence.

# Example sentence
sentence = "Barack Obama was born in Hawaii and was elected president in 2008."
doc = nlp(sentence)
for ent in doc.ents:
    print(ent.text, ent.label_)

### 4. Converting Text to Numbers
#### Use CountVectorizer and TfidfVectorizer to convert a list of sentences into numeric vectors.

sentences = [
    "I love machine learning.",
    "Natural language processing is a part of AI.",
    "AI is the future."
]

# CountVectorizer
count_vec = CountVectorizer()
X_count = count_vec.fit_transform(sentences)
print("Count Vectorizer Output:\n", X_count.toarray())

# TfidfVectorizer
tfidf_vec = TfidfVectorizer()
X_tfidf = tfidf_vec.fit_transform(sentences)
print("\nTF-IDF Vectorizer Output:\n", X_tfidf.toarray())

### 5. Word Embeddings (Advanced)
#### Use spaCy to get word vectors (embeddings) for given words.

# Note: en_core_web_sm does not have word vectors. You can install and use en_core_web_md
# Uncomment below to install and load the medium model if needed.
# !python -m spacy download en_core_web_md
# nlp = spacy.load("en_core_web_md")

# Example word vector
word = nlp("machine")[0]
print("Vector for 'machine':\n", word.vector)

def get_word_vectors(words):
    vectors = {}
    for word in words:
        # Process the word and get the first token's vector
        doc = nlp(word)
        vectors[word] = doc[0].vector
    return vectors

# Example usage with words
words = ["machine", "learning", "science"]
word_vectors = get_word_vectors(words)

# Print vectors
for word, vector in word_vectors.items():
    print(f"Vector for '{word}':")
    print(vector)
    print(f"Vector dimension: {vector.shape}\n")
